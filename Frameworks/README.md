# Прикладные системы и фреймворки искусственного интеллекта

Репозиторий содержит результаты пяти лабораторных работ по курсу «Прикладные системы и фреймворки искусственного интеллекта». Каждая лабораторная работа выполнена в формате Jupyter Notebook с подробными комментариями к каждой ячейке.

---

## Выполнил: Кострюков Евгений, студент группы М8О-407Б-22

---

## Содержание репозитория
- **`notebooks/`** - папка с Jupyter Notebooks:
  - `lab1.ipynb` - KNN: классификация и регрессия;
  - `lab2.ipynb` - Логистическая и линейная регрессия;
  - `lab3.ipynb` - Решающие деревья;
  - `lab4.ipynb` - Случайный лес;
  - `lab5.ipynb` - Градиентный бустинг и итоговое сравнение алгоритмов.
- **`data/`** - папка с исходными наборами данных: 
  - `diabetes_dataset.csv` - для классификации;  
  - `House_Price_Prediction.csv` - для регрессии.  

---

## Описание датасетов

### 1) Классификация: *Diabetes Prediction Dataset*
- Короткое описание: бинарная классификация - определение наличия диабета по медицинским показателям (возраст, пол, индекс массы тела, глюкоза, HbA1c и т.д.);
- Источник: [Kaggle — Diabetes Prediction Dataset](https://www.kaggle.com/datasets/priyamchoksi/100000-diabetes-clinical-dataset)
- Используемые файлы: `data/diabetes_dataset.csv` (100000 строк, 16 параметров).

### 2) Регрессия: *House Price Prediction*
- Короткое описание: предсказание цены недвижимости на основе характеристик объявления и объекта;
- Источник: [Kaggle — House Price Prediction Challenge](https://www.kaggle.com/datasets/anmolkumar/house-price-prediction-challenge)
- Используемые файлы: `data/House_Price_Prediction.csv` (29451 строк, 12 столбцов).

---

## Краткое содержание каждой лабораторной

### ЛР1 (`notebooks/lab1.ipynb`)
- Выбор наборов данных (классификация и регрессия) и обоснование выбора;
- Построение бейзлайна: обучение моделей sklearn (KNeighborsClassifier и KNeighborsRegressor), оценка метрик;
- Формирование гипотез по улучшению: масштабирование, создание новых признаков (временные — hour/day_of_week/month для классификации; LOG_SQFT и SQFT_PER_ROOM для регрессии), подбор гиперпараметра k на кросс-валидации;
- Улучшенный бейзлайн: обучение и сравнение метрик;
- Собственная имплементация MyKNNClassifier и MyKNNRegressor (с нуля), обучение и сравнение с sklearn;
- В ноутбуке присутствуют комментарии минимум к каждой ячейке и выводы по каждому пункту.

### ЛР2 (`notebooks/lab2.ipynb`)
- Повтор пунктов 2-4 из ЛР1, но с использованием LogisticRegression (для классификации) и LinearRegression (для регрессии).

### ЛР3 - Дерево решений (`notebooks/lab3.ipynb`)
- Повтор пунктов 2–4 из ЛР1 с DecisionTreeClassifier и DecisionTreeRegressor.

### ЛР4 (`notebooks/lab4.ipynb`)
- Повтор пунктов 2–4 из ЛР1 с RandomForestClassifier/Regressor.

### ЛР5 (`notebooks/lab5.ipynb`)
- Повтор пунктов 2–4 из ЛР1 с GradientBoosting;
- Итоговое сравнение всех алгоритмов (KNN, Logistic/Linear, Decision Tree, Random Forest, Gradient Boosting).

---

## Краткие выводы по ЛР1
- **Классификация**:
  - Baseline KNN на исходных данных показал высокую Accuracy = 0.957, но при этом Recall оставался умеренным (0.56), что связано с дисбалансом классов и зависимостью KNN от масштаба признаков;
  - Масштабирование числовых признаков дало минимальные изменения метрик, что ожидаемо для KNN - расстояния меняются, но структура данных остаётся прежней;
  - Добавление категориальных признаков (BMI class, HbA1c class) привело к умеренному росту Recall (с 0.56 до 0.58) и увеличению F1-score. Это говорит о том, что новые признаки действительно несут полезную информацию;
  - Подбор оптимального k через кросс-валидацию показал, что оптимальное значение K = 9, что слегка отличается от baseline K = 5. Изменение числа соседей в данном случае также не привело к значительным улучшениям, так как датасет большой и классы относительно хорошо разделяются;
  - Собственная реализация MyKNNClassifier полностью совпала по метрикам со sklearn-версией, что подтверждает корректность имплементации и правильно выбранную обработку входных данных.

- **Регрессия**:
  - Baseline KNN показал умеренное качество (R^2 = 0.59, MAE = 66.8);
  - Добавление информативных признаков (LOG_SQFT, SQFT_PER_ROOM) и масштабирование числовых данных повысили R^2 до 0.67 и снизили MAE до 47;
  - Собственная реализация `MyKNNRegressor` совпала по качеству с sklearn KNN, что подтверждает корректность работы алгоритма на улучшенных данных.

---

## Краткие выводы по ЛР2
- **Логистическая регрессия (классификация)**:
  - Улучшение признаков (масштабирование + BMI/HbA1c в виде категорий) дало заметный рост качества. Подбор параметра C слегка улучшил F1. Собственная реализация логистической регрессии показала такие же результаты, что подтверждает корректность алгоритма.

- **Линейная регрессия (регрессия)**:
  - Baseline работал слабее из-за мультиколлинеарности и немасштабированных данных. Добавление новых признаков и удаление лишних резко улучшило качество (R² вырос до ~0.67). Ridge почти не изменил метрики. Собственная реализация полностью совпала с sklearn.

---

## Краткие выводы по ЛР3
- **Классификация (DecisionTreeClassifier)**:
  - Baseline дерево показало высокую Accuracy (0.956) и более высокий Recall, чем логистическая регрессия, что важно для медицинской задачи. Подбор глубины и min_samples_leaf значительно улучшил модель: F1 вырос с 0.74 до 0.805, Precision стал равен 1.0, что снизило переобучение и сделало модель более устойчивой. Собственная реализация дерева полностью совпала по метрикам с sklearn, что подтверждает корректность имплементации.

- **Регрессия (DecisionTreeRegressor)**:
  - Дерево значительно превзошло линейную регрессию: MAE снизился с 131 до 39, RMSE - с 569 до 350, а R^2 вырос с 0.41 до 0.78. Проверенные гипотезы (новые признаки, ограничение глубины, настройка параметров узлов) дали минимальное улучшение или лёгкое ухудшение - модель уже была близка к максимуму качества, и дальнейшие локальные изменения практически не влияют на метрики.

---

## Краткие выводы по ЛР4



---
## Краткие выводы по ЛР5


---

## Инструкция по запуску

Так как ноутбуки выполнялись в **Google Colab**, запуск происходит следующим образом:

1. Перейдите в [Google Colab](https://colab.research.google.com/).  
2. Нажмите **File -> Upload notebook** и выберите нужный файл (`lab1.ipynb`, `lab2.ipynb` и т.д.) из папки `notebooks/`.  
3. Для работы с данными:
   - Скачайте файлы `diabetes_dataset.csv` и `House_Price_Prediction.csv` из папки `data/` или с Kaggle;
   - Загрузите их в Colab через левое меню.
4. Выполните все ячейки ноутбука.