\setcounter{section}{0}
\setcounter{subsection}{0}

\section{Описание}

\textbf{Цель работы:} Реализовать эффективный алгоритм разбиения текстов документов на отдельные лексемы (токены) и подготовить данные для построения поискового индекса.

\subsection{Правила токенизации}

В ходе работы был реализован токенизатор на языке \textbf{C++}, интегрированный в основной проект на Python. Выбранный подход обеспечивает высокую производительность за счет нативного выполнения кода.

\textbf{Выработанные правила:}
\begin{enumerate}
	\item \textbf{Алфавит:} токеном считается последовательность символов, состоящая из букв (кириллица, латиница) и цифр.
	\item \textbf{Разделители:} пробельные символы, знаки препинания (точки, запятые, скобки и т.д.) считаются разделителями и игнорируются.
	\item \textbf{Сложные слова:} дефис (\texttt{-}) и апостроф (\texttt{'}) считаются частью токена только если они находятся \textit{внутри} слова (например, it-индустрия). Дефисы в начале или конце слова отбрасываются.
	\item \textbf{Нормализация:} все буквы приводятся к нижнему регистру (A--Z $\to$ a--z, А--Я $\to$ а--я, Ё $\to$ ё).
	\item \textbf{Кодировка:} полная поддержка UTF-8 с корректной обработкой многобайтовых символов кириллицы.
\end{enumerate}

\textbf{Примеры применения правил:}
\begin{itemize}
	\item Full-stack разработчик $\to$ full-stack, разработчик
	\item IT-индустрия 2025 $\to$ it-индустрия, 2025
	\item Привет, мир! $\to$ привет, мир
\end{itemize}

\subsection{Статистические данные}

Анализ был проведен на полном корпусе документов (Habr + Lenta), собранном в предыдущих лабораторных работах.

\begin{longtable}{|p{11cm}|p{4cm}|}
	\hline
	\textbf{Метрика} & \textbf{Значение} \\
	\hline
	Количество документов & 41\,684 \\
	\hline
	Общий объем обработанного текста & $\sim$417 МБ (входные данные) \\
	\hline
	Общее количество токенов & 59\,300\,082 \\
	\hline
	Количество уникальных токенов & 1\,081\,897 \\
	\hline
	Средняя длина токена & 5.83 символа \\
	\hline
\end{longtable}

\textit{Примечание:} средняя длина токена (5.83) является характерной для русскоязычных текстов публицистического и технического стиля.

\textbf{Топ-10 частых токенов:}
\begin{itemize}
	\item в: 1\,628\,439
	\item и: 1\,411\,364
	\item на: 809\,340
	\item не: 529\,180
	\item с: 525\,239
	\item для: 506\,586
	\item что: 404\,500
	\item а: 330\,068
	\item по: 286\,643
	\item как: 285\,358
\end{itemize}

\subsection{Производительность и скорость}

Измерения времени выполнения показали следующие результаты:
\begin{itemize}
	\item \textbf{Время токенизации (C++ core):} 5.38 сек.
	\item \textbf{Полное время (включая чтение из БД):} 109.68 сек.
	\item \textbf{Скорость токенизации:} $\sim$130\,512 КБ/сек ($\sim$127 МБ/сек).
\end{itemize}

\textbf{Зависимость от объема данных.}\\
Алгоритм является однопроходным, сложность составляет O(N), где N --- количество символов во входном тексте. Время выполнения растет линейно с увеличением объема данных.

\textbf{Оценка оптимальности.}\\
Скорость в $\sim$127 МБ/сек для однопоточного приложения является близкой к оптимальной (ограничена скоростью работы с памятью и аллокациями строк).

\textbf{Пути ускорения:}
\begin{enumerate}
	\item \textbf{Многопоточность:} распараллеливание обработки документов (MapReduce подход) позволит утилизировать все ядра CPU.
	\item \textbf{SIMD-инструкции:} использование векторных инструкций для проверки символов и смены регистра.
	\item \textbf{Батчинг:} чтение данных из БД более крупными блоками для снижения накладных расходов на I/O.
\end{enumerate}

\subsection{Проблемные токены}

\begin{quotation}
	\textbf{1. Числа} (129 уникальных)\\
	Примеры: 3366, 52, 2024, 05, 500\\
	Часто бесполезны для текстового поиска, нужна фильтрация или отдельная индексация.
	
	\textbf{2. Короткие токены $\le$ 2 символов} (164 уникальных)\\
	Примеры: ии, ту, но, нг, 4k, 1k\\
	Среди них --- служебные слова и шум. Решение: \texttt{min\_token\_length = 3}.
	
	\textbf{3. Смешение алфавитов} (10 уникальных)\\
	Примеры: ai-чатботы, it-индустрии, vps-хостинг, l-тирозин\\
	Не всегда ошибка (термины, бренды), но усложняет аналитику.
\end{quotation}

\subsection{Возможные улучшения}

\begin{quotation}
	\textbf{1. Фильтрация чисел:}\\
	Отбрасывать токены или выделять в отдельный канал.
	
	\textbf{2. Минимальная длина = 3:}\\
	Убрать \enquote{в}, \enquote{и}, \enquote{с}, \enquote{на} (либо стоп-слова).
	
	\textbf{3. Ограничение \texttt{max\_token\_length = 25}:}\\
	Отсечь \enquote{мусор} из склеенной разметки.
	
	\textbf{4. Разделение дефисов (опционально):}\\
	it-индустрии $\to$ it, индустрии для более тонкой морфологии.
\end{quotation}

\pagebreak

\section{Исходный код}

Проект построен по гибридной архитектуре, объединяющей производительность низкоуровневого языка и удобство скриптового управления. Ядро токенизатора написано на \textbf{C++} и скомпилировано в динамическую библиотеку (\texttt{.dll / .so}), которая загружается в Python через модуль \texttt{ctypes}.

\subsection{Структура реализации}

Файловая организация проекта:
\begin{itemize}
	\item \texttt{cpp/tokenizer.cpp} — реализация класса Tokenizer; содержит основную логику конечного автомата для разбора UTF-8 строк.
	\item \texttt{cpp/tokenizer\_lib.cpp} — экспорт функций в стиле C (\texttt{extern "C"}) для обеспечения бинарной совместимости с Python.
	\item \texttt{python/tokenizer\_wrapper.py} — Python-обертка, инкапсулирующая работу с памятью и вызовы C-функций.
	\item \texttt{main.py} — скрипт-оркестратор: выгружает данные из MongoDB, передает их в токенизатор и сохраняет результаты.
\end{itemize}

\subsection{Ключевые алгоритмы}

\textbf{1. Проверка символов (is\_valid\_token\_char).}\\
Одной из ключевых задач было корректное определение допустимых символов в UTF-8. Была реализована собственная легковесная проверка диапазонов Unicode кодов.

\textit{Example from cpp/tokenizer.cpp}

\begin{lstlisting}[language=C++]
	bool is_valid_token_char(uint32_t cp) {
		// Latin letters (a-z, A-Z) and digits
		if ((cp >= 'a' && cp <= 'z') || (cp >= 'A' && cp <= 'Z') || (cp >= '0' && cp <= '9'))
		return true;
		
		// Cyrillic (basic range + Yo)
		if ((cp >= 0x0410 && cp <= 0x044F) || cp == 0x0401 || cp == 0x0451)
		return true;
		
		return false;
	}
\end{lstlisting}

\textbf{2. Конечный автомат разбора.}\\
Токенизация происходит за один проход по строке. Алгоритм накапливает символы в буфер \texttt{current\_token}, пока встречает допустимые символы или внутрисловные разделители (дефис). При встрече разделителя накопленный токен сбрасывается в список результатов.

\textit{Parsing logic fragment}

\begin{lstlisting}[language=C++]
	if (is_valid_token_char(codepoint)) {
		// Append char to current token with lowercase conversion
		utf8::append(to_lower(codepoint), current_token);
	}
	else if (codepoint == '-' || codepoint == '\'') {
		// Keep hyphen inside token if token is not empty
		if (!current_token.empty()) {
			utf8::append(codepoint, current_token);
		}
	}
	else {
		// Separator met: finalize current token
		if (!current_token.empty()) {
			trim_trailing_hyphens(current_token);
			if (!current_token.empty()) {
				tokens.push_back(current_token);
			}
			current_token.clear();
		}
	}
\end{lstlisting}

\textbf{3. Интеграция с Python (ctypes).}\\
Для передачи данных между Python и C++ используется прямая работа с указателями, что позволяет избежать лишнего копирования больших массивов текста.

\textit{Fragment from python/tokenizer\_wrapper.py}

\begin{lstlisting}[language=Python]
	class TokenizerWrapper:
	
	def tokenize(self, text: str) -> List[str]:
	# Convert Python string to UTF-8 bytes
	utf8_data = text.encode('utf-8')
	
	# Call C++ function
	# Result is returned as a delimiter-separated byte string
	self.lib.tokenize_text(
	utf8_data,
	len(utf8_data),
	result_buffer,
	byref(result_len)
	)
	
	# ... parse result ...
\end{lstlisting}

\pagebreak